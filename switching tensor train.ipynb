{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensor Switching Networks](https://arxiv.org/pdf/1610.10087v1.pdf) (sort of) generalise relus. The intuition derives from writing a relu layer's output as:\n",
    "$$\n",
    "    X_l = \\left(H(W_{l}X_{l-1}) \\otimes X_{l-1} \\odot W_{l}\\right) \\times \\mathbf{1}^{n_0}\n",
    "$$\n",
    "\n",
    "Where $X_i$ is the activation for layer $i$, $W_i$ the weights, $H(\\cdot)$ is the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function) applied elementwise,\n",
    "$\\odot$ is a kind of broadcasted tensor-matrix Hadamard where we do it for each slice of the tensor. The vector of ones at the end squishes it back down the right shape.\n",
    "\n",
    "Tensor switching networks simplify slightly by skipping the projection at the end:\n",
    "$$\n",
    "    Z_l = H(W_lX_{l-1}) \\otimes Z_{l-1}\n",
    "$$\n",
    "ensuring $Z_0 = X_0$ is the input to the network. Essentially this just gets gated in complex ways as the tensor grows (with a new dimension for each layer). Finally, just flatten the thing and send it into a standard layer.\n",
    "\n",
    "There are two issues with this:\n",
    "1. the tensor gets very large\n",
    "    - because it adds a whole dimension at each layer (it is likely to be sparse and what is there is just copies).\n",
    "1. it is impossible to train the weights with normal backprop\n",
    "    - because $H$ has zero gradients (where it is defined).\n",
    "\n",
    "The authors offer the following solutions:\n",
    "1. nothing\n",
    "2. some crazy schemes including:\n",
    "    - just use learn only the last layer\n",
    "    - use nearly the gradients you would get if it were a normal relu net\n",
    "    - pretty much changing everything into a different kind of network (which is interesting in its own right)\n",
    "    \n",
    "    \n",
    "Here we propose to investigate 2 ideas to solve these issues:\n",
    "1. use the tensor train format\n",
    "2. use [this](https://arxiv.org/pdf/1602.02830v3.pdf) method of pretending to back-propagate\n",
    "   through binarisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor train tensor products.\n",
    "\n",
    "What does it mean to do the tensor product of a vector with a tensor in the tt-format.\n",
    "Let $A$ be a tensor in the tensor-train with elements\n",
    "$$\n",
    "    a(i_1, i_2, \\ldots, i_d) = G_1[i_1]G_2[i_2]\\cdots G_d[i_d]\n",
    "$$\n",
    "Consider $C = b \\otimes A$ for some $b \\in \\mathbb{R}^{m}$. This should be the same as making $m$ copies of $A$ and multiplying each copy by different elements of $b$. Then\n",
    "$$\n",
    "    c(i_m, i_1, i_2, \\ldots, i_d) = b_{i_m} G_1[i_1]G_2[i_2]\\cdots G_d[i_d]\n",
    "$$\n",
    "\n",
    "So this is equivalent to adding dummy dimensions to make \n",
    "$b \\in \\mathbb{R}^{1\\times m\\times 1}$ and sticking it on the end of the tensor train.\n",
    "This seems intuitive on the surface -- we increase the dimensions by adding a new core.\n",
    "It also starts to make it feel quite suspiciously like we're going to end up with a\n",
    "pretty standard looking deep net if we apply this to the above tensor switching net.\n",
    "\n",
    "#### _Tensor train tensor switching network_\n",
    "\n",
    "We start with an input vector $Z_0 = X_0$, and for each layer $l$ compute:\n",
    "$$\n",
    "X_l = H(W_lX_{l-1}) \\\\\n",
    "Z_l = X_l \\otimes Z_{l-1}\n",
    "$$\n",
    "Treating $Z_0$ as the first core of a decomposition,\n",
    "this gives the following for an element of the penultimate layer:\n",
    "$$\n",
    "Z_L(i_L, i_{L-1}, \\ldots i_1) = X_L(i_L)X_{L-1}(i_{L-1}) \\cdots X_1(i_1).\n",
    "$$\n",
    "This is what we would expect. As the X are 0,1 for a value to propagate its way to the output,\n",
    "every layer has to let it through (but there are exponentially many chances for\n",
    "this to happen). Curiously, we haven't actually changed anything here -- the decomposition\n",
    "at this stage buys us nothing as the cores being added have to have tt-rank 1.\n",
    "\n",
    "The output layer is linear and needs to see all elements of $Z_L$. As we've gone to all the\n",
    "trouble of keeping $Z_L$ in a tensor train, it makes sense to try and avoid blowing it all\n",
    "back up right at the end, especially as it turns out to be a very low-rank tensor. The answer\n",
    "here is to realise what we're actually doing -- we want a linear operation to reduce a tensor\n",
    "a vector. We want to therefore contract away all the dimensions except one and then project\n",
    "that to the right size. This should be able to be realised in a single tensor-tensor product\n",
    "(which should turn out to be equivalent to flattening and doing a matrix product). If we \n",
    "recognise the tensor-tensor nature of this final layer, we can use a low-rank tensor train\n",
    "for the output weights and avoid horrible exponential sizes.\n",
    "\n",
    "Introduce a final weight tensor $U \\in \\mathbb{R}^{t, n_L \\times \\cdots \\times n_1}$ with a\n",
    "dimension per layer and an extra output dimension of size $t$. \n",
    "Denoting the output of the network $\\hat{Y}$, a single element should be\n",
    "$$\n",
    "\\hat{Y}_k = \\sum_{i_L, \\ldots i_1} U(k, i_L, \\ldots, i_1)Z_L(i_L, \\ldots, i_1)\n",
    "$$ (which affirms the intuition that it is the same as flattening an using matrices).\n",
    "\n",
    "Recalling the definition of $Z_l$ and letting $U$ be in the tt-format with $L+1$ cores $G_i$,\n",
    "$$\n",
    "\\hat{Y}_k = \\sum_{i_L, \\ldots i_1} G_{L+1}(k)G_{L}(i_L)\\cdots G_{1}(i_1) X_L(i_L)\\cdots X_1(i_1).\n",
    "$$ Seeing as the $X_j(i_j)$ are all in fact scalars, we can rearrange:\n",
    "$$\n",
    "\\hat{Y}_k = G_{L+1}(k) \\left(\\sum_{i_L}G_{L}(i_L) X_L(i_L)\\right) \\cdots \\left(\\sum_{i_1}G_1(i_1)\\right).\n",
    "$$ As $G_{L+1}(k)$ is a row vector, $G_{1}(i_1)$ is a column vector and the rest of the $G$\n",
    "slices are matrices the shapes work out nicely.\n",
    "\n",
    "We should be able now to define the whole network in one go -- the _analysis_ weights $U$ get\n",
    "folded in to each layer. This means each layer $l$ essentially has two outputs: the running\n",
    "product of the reduced $G_l$s (which we term $Y_l$ \n",
    "and the most recent activations $X_l$. The activations are used\n",
    "to compute the next set of activations, which are used to reduce the current $G_l$ which is in\n",
    "turn used to produce the output $Y_l$. Formally:\n",
    "$$\n",
    "X_l = H(W_lX_{l-1}) \\\\\n",
    "Y_l = \\left(\\sum_{i_l}G_l(i_l)X_l(i_l)\\right)Y_{l-1}\n",
    "$$\n",
    "and finally\n",
    "$$\n",
    "\\hat{Y} = G_{L+1} Y_L\n",
    "$$ (technically $G_{L+1}$ is a 3-tensor, but one dimension is 1 so here we treat it as a matrix).\n",
    "\n",
    "Probably it will turn out to be a good plan to add biases to the linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training\n",
    "\n",
    "We have a network, it's kind of nice. At each stage, we make some binary masks and use them to\n",
    "select slices of the $G$ to combine. The $G$ have a nice linear progression all the way to the\n",
    "input. The second key issue is still unresolved: how to train the $W_l$. Clearly the $G_l$\n",
    "should be fine with normal backprop, but the Heaviside step function has zero gradients.\n",
    "Therefore $W_l$ will never learn and we are doomed.\n",
    "\n",
    "Tsai, Saxe and Cox propose few methods, the only one which appears to remain relevant involves\n",
    "computing the pseudoinverse of the gradients of auxiliary variables to derive a\n",
    "pseudo-gradient for the problematic variables. This seems somewhat unpleasant, so it might\n",
    "be preferable to avoid it if possible.\n",
    "\n",
    "An alternative approach for backpropagating through binarising input functions is presented\n",
    "by [Courbariaux et al.](https://arxiv.org/pdf/1602.02830v3.pdf) (although not for the first\n",
    "time). They are using a sign function\n",
    "$$\n",
    "    q = S(r) = \\begin{cases} 1 & r > 0 \\\\\n",
    "    -1 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$ to binarize\n",
    "and use a pseudo-gradient referred to as the _straight through_ estimator (note that it\n",
    "is not an estimator -- the gradients are 0 or undefined, this does not require estimation).\n",
    "Using $S^*$ in the place of $S'$ to denote this false gradient, this approach defines:\n",
    "$$\n",
    "    S^*(r) = 1_{|r| \\leq 1}\n",
    "$$\n",
    "That is to say: a gradient of 1 unless the input is outside a certain range. This is precisely\n",
    "the gradient of the hard tanh function $h(r) = max(-1, min(1, r))$.\n",
    "\n",
    "The step function we are intending to use is very similar to the sign function above -- the\n",
    "only difference is that the lowest value it outputs is zero. This suggests we may succeed with\n",
    "a similar pseudo-gradient, so it is worth a shot. As the threshold is in the same place, there\n",
    "is probably no need to adjust the above.\n",
    "\n",
    "An alternative is to relax the decision function into something that does have a \n",
    "(sub-)gradient. One answer might be a sigmoid, although these are notoriously poor in deep\n",
    "feed-forward nets. An answer which retains a lot of desirable properties might be a variation\n",
    "on the hard tanh above: $s(r) = max(0, min(1, r))$. This is a kind of hard sigmoid, without\n",
    "the scaling usually applied. The scaling and shifting of the input is omitted for two reasons:\n",
    "firstly, if it is helpful the network should be able to learn it and secondly it maintains the\n",
    "same boundary as the heaviside step function -- any input less than zero will output zero.\n",
    "\n",
    "This hard sigmoid has a non-zero gradient when the input is between 0 and 1, so we may be able\n",
    "to learn something. Let's go ahead and implement both of these and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# had to implement the fake gradient activation as its own op\n",
    "from ops import binary_connect\n",
    "from functools import partial\n",
    "\n",
    "bc_01 = partial(binary_connect, min_=0.0)\n",
    "\n",
    "from rnndatasets import sequentialmnist as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hard_sigmoid(activations):\n",
    "    return tf.maximum(0.0, tf.minimum(1.0, activations))\n",
    "\n",
    "def ttts_layer(previous_activations, previous_output, hidden_units, tensor_rank,\n",
    "               nonlin=hard_sigmoid, scope=None):\n",
    "    \"\"\"Makes a layer as described above. Tensor rank gets to only be a single integer\n",
    "    because previous_output should be [batch_size, previous_rank]\"\"\"\n",
    "    with tf.variable_scope(scope or 'ttts_layer'):\n",
    "        # first compute activations\n",
    "        input_shape = previous_activations.get_shape()[1].value\n",
    "        weights = tf.get_variable('weights', shape=[input_shape, hidden_units])\n",
    "        # give the biases a high-ish init and it will start off basically linear\n",
    "        biases = tf.get_variable('biases', shape=[hidden_units],\n",
    "                                 initializer=tf.constant_initializer(1.0))\n",
    "        acts = tf.nn.bias_add(tf.matmul(previous_activations, weights), biases)\n",
    "        acts = nonlin(acts)\n",
    "        \n",
    "        # now get the tt-core of the output tensor and do a bilinear product with\n",
    "        # previous_output and acts\n",
    "        prev_rank = previous_output.get_shape()[1].value\n",
    "        # we're going to do this all in terms of flattenings, so that we can use the\n",
    "        # nice big matmuls. The downside is there's a bit of exra storage\n",
    "        core = tf.get_variable('core', shape=[prev_rank*hidden_units, tensor_rank])\n",
    "        # if we take the outer product of previous_output and hidden_units then\n",
    "        # we can just multiply by the above. Regrettably this can be quite large.\n",
    "        # There's possibly a better way, but that can wait\n",
    "        outer = tf.batch_matmul(tf.expand_dims(previous_output, 2),\n",
    "                                tf.expand_dims(acts, 1))\n",
    "        outer = tf.reshape(outer, [-1, prev_rank*hidden_units])\n",
    "        output = tf.matmul(outer, core)\n",
    "        \n",
    "    return acts, output\n",
    "\n",
    "def get_full_net(input_var, num_outputs, depth, width, rank, nonlin=hard_sigmoid,\n",
    "                 scope=None):\n",
    "    \"\"\"Gets the full network. Not terribly flexible, all layers are the same\n",
    "    width and all of the TT cores are the same rank (except where they have\n",
    "    to be 1)\"\"\"\n",
    "    with tf.variable_scope(scope or 'ttttttttsnettt'):\n",
    "        activations, outputs = input_var, input_var\n",
    "        for layer in range(depth):\n",
    "            activations, outputs = ttts_layer(activations, outputs,\n",
    "                                              width, rank, nonlin,\n",
    "                                              'layer_{}'.format(layer))\n",
    "        # now the last layer is just a linear transform\n",
    "        # should be rank -> num_outputs\n",
    "        with tf.variable_scope('output_layer'):\n",
    "            weights = tf.get_variable('weights', [rank, num_outputs])\n",
    "            biases = tf.get_variable('biases', [num_outputs])\n",
    "            result = tf.nn.bias_add(tf.matmul(outputs, weights), biases)\n",
    "        return result\n",
    "    \n",
    "def count_params(scope):\n",
    "    return np.sum(\n",
    "        [np.prod([dim for dim in var.get_shape().as_list()]) \n",
    "         for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard sigmoid: 84640 parameters\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "input_pl = tf.placeholder(tf.float32, shape=[BATCH_SIZE, 728], name='inputs')\n",
    "target_pl = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name='targets')\n",
    "\n",
    "with tf.variable_scope('hs_net') as scope:\n",
    "    hs_net_out = get_full_net(input_pl, 10, 5, 10, 10, scope=scope)\n",
    "    print('hard sigmoid: {} parameters'.format(count_params(scope.name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
