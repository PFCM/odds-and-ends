{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensor Switching Networks](https://arxiv.org/pdf/1610.10087v1.pdf) (sort of) generalise relus. The intuition derives from writing a relu layer's output as:\n",
    "$$\n",
    "    X_l = \\left(H(W_{l}X_{l-1}) \\otimes X_{l-1} \\odot W_{l}\\right) \\times \\mathbf{1}^{n_0}\n",
    "$$\n",
    "\n",
    "Where $X_i$ is the activation for layer $i$, $W_i$ the weights, $H(\\cdot)$ is the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function) applied elementwise,\n",
    "$\\odot$ is a kind of broadcasted tensor-matrix Hadamard where we do it for each slice of the tensor. The vector of ones at the end squishes it back down the right shape.\n",
    "\n",
    "Tensor switching networks simplify slightly by skipping the projection at the end:\n",
    "$$\n",
    "    Z_l = H(W_lX_{l-1}) \\otimes Z_{l-1}\n",
    "$$\n",
    "ensuring $Z_0 = X_0$ is the input to the network. Essentially this just gets gated in complex ways as the tensor grows (with a new dimension for each layer). Finally, just flatten the thing and send it into a standard layer.\n",
    "\n",
    "There are two issues with this:\n",
    "1. the tensor gets very large\n",
    "    - because it adds a whole dimension at each layer (it is likely to be sparse and what is there is just copies).\n",
    "1. it is impossible to train the weights with normal backprop\n",
    "    - because $H$ has zero gradients (where it is defined).\n",
    "\n",
    "The authors offer the following solutions:\n",
    "1. nothing\n",
    "2. some crazy schemes including:\n",
    "    - just use learn only the last layer\n",
    "    - use nearly the gradients you would get if it were a normal relu net\n",
    "    - pretty much changing everything into a different kind of network (which is interesting in its own right)\n",
    "    \n",
    "    \n",
    "Here we propose to investigate 2 ideas to solve these issues:\n",
    "1. use the tensor train format\n",
    "2. use [this](https://arxiv.org/pdf/1602.02830v3.pdf) method of pretending to back-propagate\n",
    "   through binarisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor train tensor products.\n",
    "\n",
    "What does it mean to do the tensor product of a vector with a tensor in the tt-format.\n",
    "Let $A$ be a tensor in the tensor-train with elements\n",
    "$$\n",
    "    a(i_1, i_2, \\ldots, i_d) = G_1[i_1]G_2[i_2]\\cdots G_d[i_d]\n",
    "$$\n",
    "Consider $C = b \\otimes A$ for some $b \\in \\mathbb{R}^{m}$. This should be the same as making $m$ copies of $A$ and multiplying each copy by different elements of $b$. Then\n",
    "$$\n",
    "    c(i_m, i_1, i_2, \\ldots, i_d) = b_{i_m} G_1[i_1]G_2[i_2]\\cdots G_d[i_d]\n",
    "$$\n",
    "\n",
    "So this is equivalent to adding dummy dimensions to make \n",
    "$b \\in \\mathbb{R}^{1\\times m\\times 1}$ and sticking it on the end of the tensor train.\n",
    "This seems intuitive on the surface -- we increase the dimensions by adding a new core.\n",
    "It also starts to make it feel quite suspiciously like we're going to end up with a\n",
    "pretty standard looking deep net if we apply this to the above tensor switching net.\n",
    "\n",
    "#### _Tensor train tensor switching network_\n",
    "\n",
    "We start with an input vector $Z_0 = X_0$, and for each layer $l$ compute:\n",
    "$$\n",
    "X_l = H(W_lX_{l-1}) \\\\\n",
    "Z_l = X_l \\otimes Z_{l-1}\n",
    "$$\n",
    "Treating $Z_0$ as the first core of a decomposition,\n",
    "this gives the following for an element of the penultimate layer:\n",
    "$$\n",
    "Z_L(i_L, i_{L-1}, \\ldots i_1) = X_L(i_L)X_{L-1}(i_{L-1}) \\cdots X_1(i_1).\n",
    "$$\n",
    "This is what we would expect. As the X are 0,1 for a value to propagate its way to the output,\n",
    "every layer has to let it through (but there are exponentially many chances for\n",
    "this to happen). Curiously, we haven't actually changed anything here -- the decomposition\n",
    "at this stage buys us nothing as the cores being added have to have tt-rank 1.\n",
    "\n",
    "The output layer is linear and needs to see all elements of $Z_L$. As we've gone to all the\n",
    "trouble of keeping $Z_L$ in a tensor train, it makes sense to try and avoid blowing it all\n",
    "back up right at the end, especially as it turns out to be a very low-rank tensor. The answer\n",
    "here is to realise what we're actually doing -- we want a linear operation to reduce a tensor\n",
    "a vector. We want to therefore contract away all the dimensions except one and then project\n",
    "that to the right size. This should be able to be realised in a single tensor-tensor product\n",
    "(which should turn out to be equivalent to flattening and doing a matrix product). If we \n",
    "recognise the tensor-tensor nature of this final layer, we can use a low-rank tensor train\n",
    "for the output weights and avoid horrible exponential sizes.\n",
    "\n",
    "Introduce a final weight tensor $U \\in \\mathbb{R}^{t, n_L \\times \\cdots \\times n_1}$ with a\n",
    "dimension per layer and an extra output dimension of size $t$. \n",
    "Denoting the output of the network $\\hat{Y}$, a single element should be\n",
    "$$\n",
    "\\hat{Y}_k = \\sum_{i_L, \\ldots i_1} U(k, i_L, \\ldots, i_1)Z_L(i_L, \\ldots, i_1)\n",
    "$$ (which affirms the intuition that it is the same as flattening an using matrices).\n",
    "\n",
    "Recalling the definition of $Z_l$ and letting $U$ be in the tt-format with $L+1$ cores $G_i$,\n",
    "$$\n",
    "\\hat{Y}_k = \\sum_{i_L, \\ldots i_1} G_{L+1}(k)G_{L}(i_L)\\cdots G_{1}(i_1) X_L(i_L)\\cdots X_1(i_1).\n",
    "$$ Seeing as the $X_j(i_j)$ are all in fact scalars, we can rearrange:\n",
    "$$\n",
    "\\hat{Y}_k = G_{L+1}(k) \\left(\\sum_{i_L}G_{L}(i_L) X_L(i_L)\\right) \\cdots \\left(\\sum_{i_1}G_1(i_1)\\right).\n",
    "$$ As $G_{L+1}(k)$ is a row vector, $G_{1}(i_1)$ is a column vector and the rest of the $G$\n",
    "slices are matrices the shapes work out nicely.\n",
    "\n",
    "We should be able now to define the whole network in one go -- the _analysis_ weights $U$ get\n",
    "folded in to each layer. This means each layer $l$ essentially has two outputs: the running\n",
    "product of the reduced $G_l$s (which we term $Y_l$ \n",
    "and the most recent activations $X_l$. The activations are used\n",
    "to compute the next set of activations, which are used to reduce the current $G_l$ which is in\n",
    "turn used to produce the output $Y_l$. Formally:\n",
    "$$\n",
    "X_l = H(W_lX_{l-1}) \\\\\n",
    "Y_l = \\left(\\sum_{i_l}G_l(i_l)X_l(i_l)\\right)Y_{l-1}\n",
    "$$\n",
    "and finally\n",
    "$$\n",
    "\\hat{Y} = G_{L+1} Y_L\n",
    "$$ (technically $G_{L+1}$ is a 3-tensor, but one dimension is 1 so here we treat it as a matrix).\n",
    "\n",
    "Probably it will turn out to be a good plan to add biases to the linear transformations.\n",
    "\n",
    "Note that each layer is performing a _bilinear product_ between the current $X$, the previous\n",
    "$Y$ and the 3-tensor core $G$. We could reduce parameters & computation significantly by\n",
    "storing the cores themselves in a decomposition -- the CP is good for bilinear products. For\n",
    "now though, we will just train relatively small nets and deal with the product in its full\n",
    "glory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training\n",
    "\n",
    "We have a network, it's kind of nice. At each stage, we make some binary masks and use them to\n",
    "select slices of the $G$ to combine. The $G$ have a nice linear progression all the way to the\n",
    "input. The second key issue is still unresolved: how to train the $W_l$. Clearly the $G_l$\n",
    "should be fine with normal backprop, but the Heaviside step function has zero gradients.\n",
    "Therefore $W_l$ will never learn and we are doomed.\n",
    "\n",
    "Tsai, Saxe and Cox propose few methods, the only one which appears to remain relevant involves\n",
    "computing the pseudoinverse of the gradients of auxiliary variables to derive a\n",
    "pseudo-gradient for the problematic variables. This seems somewhat unpleasant, so it might\n",
    "be preferable to avoid it if possible.\n",
    "\n",
    "An alternative approach for backpropagating through binarising input functions is presented\n",
    "by [Courbariaux et al.](https://arxiv.org/pdf/1602.02830v3.pdf) (although not for the first\n",
    "time). They are using a sign function\n",
    "$$\n",
    "    q = S(r) = \\begin{cases} 1 & r > 0 \\\\\n",
    "    -1 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$ to binarize\n",
    "and use a pseudo-gradient referred to as the _straight through_ estimator (note that it\n",
    "is not an estimator -- the gradients are 0 or undefined, this does not require estimation).\n",
    "Using $S^*$ in the place of $S'$ to denote this false gradient, this approach defines:\n",
    "$$\n",
    "    S^*(r) = 1_{|r| \\leq 1}\n",
    "$$\n",
    "That is to say: a gradient of 1 unless the input is outside a certain range. This is precisely\n",
    "the gradient of the hard tanh function $h(r) = max(-1, min(1, r))$.\n",
    "\n",
    "The step function we are intending to use is very similar to the sign function above -- the\n",
    "only difference is that the lowest value it outputs is zero. This suggests we may succeed with\n",
    "a similar pseudo-gradient, so it is worth a shot. As the threshold is in the same place, there\n",
    "is probably no need to adjust the above.\n",
    "\n",
    "An alternative is to relax the decision function into something that does have a \n",
    "(sub-)gradient. One answer might be a sigmoid, although these are notoriously poor in deep\n",
    "feed-forward nets. An answer which retains a lot of desirable properties might be a variation\n",
    "on the hard tanh above: $s(r) = max(0, min(1, r))$. This is a kind of hard sigmoid, without\n",
    "the scaling usually applied. The scaling and shifting of the input is omitted for two reasons:\n",
    "firstly, if it is helpful the network should be able to learn it and secondly it maintains the\n",
    "same boundary as the heaviside step function -- any input less than zero will output zero.\n",
    "\n",
    "This hard sigmoid has a non-zero gradient when the input is between 0 and 1, so we may be able\n",
    "to learn something. Let's go ahead and implement both of these and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# had to implement the fake gradient activation as its own op\n",
    "from ops import binary_connect\n",
    "from functools import partial\n",
    "\n",
    "bc_01 = partial(binary_connect, min_=0.0)\n",
    "\n",
    "from rnndatasets import sequentialmnist as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hard_sigmoid(activations):\n",
    "    return tf.maximum(0.0, tf.minimum(1.0, activations))\n",
    "\n",
    "def ttts_layer(previous_activations, previous_output, hidden_units, tensor_rank,\n",
    "               nonlin=hard_sigmoid, scope=None):\n",
    "    \"\"\"Makes a layer as described above. Tensor rank gets to only be a single integer\n",
    "    because previous_output should be [batch_size, previous_rank]\"\"\"\n",
    "    with tf.variable_scope(scope or 'ttts_layer'):\n",
    "        # first compute activations\n",
    "        input_shape = previous_activations.get_shape()[1].value\n",
    "        weights = tf.get_variable('weights', shape=[input_shape, hidden_units])\n",
    "        # give the biases a high-ish init and it will start off basically linear\n",
    "        biases = tf.get_variable('biases', shape=[hidden_units],\n",
    "                                 initializer=tf.constant_initializer(1.0))\n",
    "        acts = tf.nn.bias_add(tf.matmul(previous_activations, weights), biases)\n",
    "        acts = nonlin(acts)\n",
    "        \n",
    "        # now get the tt-core of the output tensor and do a bilinear product with\n",
    "        # previous_output and acts\n",
    "        prev_rank = previous_output.get_shape()[1].value\n",
    "        # we're going to do this all in terms of flattenings, so that we can use the\n",
    "        # nice big matmuls. The downside is there's a bit of exra storage\n",
    "        core = tf.get_variable('core', shape=[prev_rank*hidden_units, tensor_rank])\n",
    "        # if we take the outer product of previous_output and hidden_units then\n",
    "        # we can just multiply by the above. Regrettably this can be quite large.\n",
    "        # There's possibly a better way, but that can wait\n",
    "        outer = tf.batch_matmul(tf.expand_dims(previous_output, 2),\n",
    "                                tf.expand_dims(acts, 1))\n",
    "        outer = tf.reshape(outer, [-1, prev_rank*hidden_units])\n",
    "        output = tf.matmul(outer, core)\n",
    "        \n",
    "    return acts, output\n",
    "\n",
    "def get_full_net(input_var, num_outputs, depth, width, rank, nonlin=hard_sigmoid,\n",
    "                 scope=None):\n",
    "    \"\"\"Gets the full network. Not terribly flexible, all layers are the same\n",
    "    width and all of the TT cores are the same rank (except where they have\n",
    "    to be 1)\"\"\"\n",
    "    with tf.variable_scope(scope or 'ttttttttsnettt'):\n",
    "        activations, outputs = input_var, input_var\n",
    "        for layer in range(depth):\n",
    "            activations, outputs = ttts_layer(activations, outputs,\n",
    "                                              width, rank, nonlin,\n",
    "                                              'layer_{}'.format(layer))\n",
    "        # now the last layer is just a linear transform\n",
    "        # should be rank -> num_outputs\n",
    "        with tf.variable_scope('output_layer'):\n",
    "            weights = tf.get_variable('weights', [rank, num_outputs])\n",
    "            biases = tf.get_variable('biases', [num_outputs])\n",
    "            result = tf.nn.bias_add(tf.matmul(outputs, weights), biases)\n",
    "        return result\n",
    "    \n",
    "def count_params(scope):\n",
    "    return np.sum(\n",
    "        [np.prod([dim for dim in var.get_shape().as_list()]) \n",
    "         for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard sigmoid: 1120910 parameters\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "DEPTH = 4  # give it some depth, but not too much\n",
    "WIDTH = 50 # pretty narrow\n",
    "RANKS = 25 # very narrow\n",
    "\n",
    "tf.reset_default_graph()\n",
    "input_pl = tf.placeholder(tf.float32, shape=[BATCH_SIZE, 784], name='inputs')\n",
    "target_pl = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name='targets')\n",
    "\n",
    "with tf.variable_scope('hs_net') as scope:\n",
    "    # default nonlin is hard_sigmoid\n",
    "    hs_net_out = get_full_net(input_pl, 10, DEPTH, WIDTH, RANKS, scope=scope)\n",
    "    \n",
    "    print('hard sigmoid: {} parameters'.format(count_params(scope.name)))\n",
    "    hs_net_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        hs_net_out, target_pl)\n",
    "    hs_net_loss = tf.reduce_mean(hs_net_loss)\n",
    "    hs_net_acc = tf.contrib.metrics.accuracy(tf.cast(tf.argmax(hs_net_out, 1), tf.int32),\n",
    "                                             target_pl)\n",
    "    hs_net_acc = tf.reduce_mean(hs_net_acc)\n",
    "    opt = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    hs_net_train = opt.minimize(\n",
    "        hs_net_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, input_var, target_var, metrics, train_op, session):\n",
    "    \"\"\"run an epoch over the data, report metrics averaged over all the data\"\"\"\n",
    "    metric_sums = [0 for _ in metrics]\n",
    "    total_steps = 0\n",
    "    for image_batch, label_batch in data_iter:\n",
    "        feed = {input_var: image_batch,\n",
    "                target_var: label_batch}\n",
    "        results = session.run(metrics +  [train_op], feed)\n",
    "        # print some stuff so we know something is happening\n",
    "        if total_steps % 10 == 0:\n",
    "            print('\\r{:~^50}'.format(\n",
    "                    ','.join(['{:.4f}'.format(mval) for mval in results[:-1]])),\n",
    "                    end='', flush=True)\n",
    "        metric_sums = [old + new for old, new in zip(metric_sums, results[:-1])]\n",
    "        total_steps += 1\n",
    "    return [total/total_steps for total in metric_sums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised\n",
      "Epoch 0:\n",
      "~~Train: xent 0.6042817367551228, accuracy 0.8443333286667863\n",
      "~~Test: xent 1.4665421938151122, accuracy 0.6906000028550625\n",
      "Epoch 1:\n",
      "~~Train: xent 0.3644533261563629, accuracy 0.8967333301405112\n",
      "~~Test: xent 0.5023277029022575, accuracy 0.8640999940037727\n",
      "Epoch 2:\n",
      "~~Train: xent 0.28824481536401436, accuracy 0.9170833300550779\n",
      "~~Test: xent 0.3994241915596649, accuracy 0.8813999953866005\n",
      "Epoch 3:\n",
      "~~Train: xent 0.26466478634470453, accuracy 0.9232166640460491\n",
      "~~Test: xent 0.44998703652992844, accuracy 0.8738999971747399\n",
      "Epoch 4:\n",
      "~~Train: xent 0.24207905962636384, accuracy 0.9303833311299483\n",
      "~~Test: xent 0.3794989710999653, accuracy 0.8965999969840049\n",
      "Epoch 5:\n",
      "~~Train: xent 0.19386336663582673, accuracy 0.9415666649738947\n",
      "~~Test: xent 0.42816819265543016, accuracy 0.8896999976038933\n",
      "Epoch 6:\n",
      "~~Train: xent 0.1914539746903271, accuracy 0.9427999981741111\n",
      "~~Test: xent 0.35477183929644523, accuracy 0.9041999953985215\n",
      "Epoch 7:\n",
      "~~Train: xent 0.18187996664472544, accuracy 0.9462333314617475\n",
      "~~Test: xent 0.2511756828956277, accuracy 0.9354999983310699\n",
      "Epoch 8:\n",
      "~~Train: xent 0.1721550751288305, accuracy 0.9496833325425784\n",
      "~~Test: xent 0.36411026732472235, accuracy 0.9042999970912934\n",
      "Epoch 9:\n",
      "~~Train: xent 0.17973464249031773, accuracy 0.9467999984323978\n",
      "~~Test: xent 0.207285945150943, accuracy 0.9458999997377395\n",
      "Epoch 10:\n",
      "~~Train: xent 0.1572346991140512, accuracy 0.95303333217899\n",
      "~~Test: xent 0.31159319730883, accuracy 0.9218999981880188\n",
      "Epoch 11:\n",
      "~~Train: xent 0.16840879916989554, accuracy 0.9505333330233892\n",
      "~~Test: xent 0.28154187515290685, accuracy 0.9270999976992607\n",
      "Epoch 12:\n",
      "~~Train: xent 0.15514464283265017, accuracy 0.9535833315054576\n",
      "~~Test: xent 0.3394766905903816, accuracy 0.913399997651577\n",
      "Epoch 13:\n",
      "~~Train: xent 0.16215370867411064, accuracy 0.9523833320538203\n",
      "~~Test: xent 0.19018521895479354, accuracy 0.9519999992847442\n",
      "Epoch 14:\n",
      "~~Train: xent 0.14515446375454, accuracy 0.9572666655480861\n",
      "~~Test: xent 0.20863564313818642, accuracy 0.9443000003695488\n",
      "Epoch 15:\n",
      "~~Train: xent 0.14583525944229525, accuracy 0.95811666538318\n",
      "~~Test: xent 0.3826988498761784, accuracy 0.9135999980568886\n",
      "Epoch 16:\n",
      "~~Train: xent 0.14204049286364656, accuracy 0.9585499993463358\n",
      "~~Test: xent 0.22367389171893592, accuracy 0.9459999999403954\n",
      "Epoch 17:\n",
      "~~Train: xent 0.14293754586658905, accuracy 0.9581833332280318\n",
      "~~Test: xent 0.32109698430635036, accuracy 0.9219999977946282\n",
      "~~~~~~~~~~~~~~~~~~0.1734,0.9200~~~~~~~~~~~~~~~~~~~"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6d443215b8df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                               \u001b[0;34m[\u001b[0m\u001b[0mhs_net_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs_net_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# things to report back on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                               \u001b[0mhs_net_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# thing to do training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                               sess)  # session to run it all\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rEpoch {}:\\n~~Train: xent {}, accuracy {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# record for plotting later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-a224d0cc44eb>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, input_var, target_var, metrics, train_op, session)\u001b[0m\n\u001b[1;32m      6\u001b[0m         feed = {input_var: image_batch,\n\u001b[1;32m      7\u001b[0m                 target_var: label_batch}\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# print some stuff so we know something is happening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now train\n",
    "hs_train_accs, hs_test_accs = [], []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print('initialised')\n",
    "    train_data = data.get_data('train', 60000)\n",
    "    # fix the data from RNN format to feed-forward shapes\n",
    "    # it's a tuple, so redo the sequence\n",
    "    train_data = [train_data[0].reshape((-1, 784)), train_data[1]]\n",
    "    test_data = data.get_data('test', 10000)\n",
    "    test_data = [test_data[0].reshape((-1, 784)), test_data[1]]\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_iter = data.batch_iter(train_data, BATCH_SIZE, False)\n",
    "        xent, acc = run_epoch(train_iter, input_pl, target_pl,  # data\n",
    "                              [hs_net_loss, hs_net_acc],  # things to report back on\n",
    "                              hs_net_train,  # thing to do training\n",
    "                              sess)  # session to run it all\n",
    "        print('\\rEpoch {}:\\n~~Train: xent {}, accuracy {}'.format(epoch, xent, acc))\n",
    "        # record for plotting later\n",
    "        hs_train_accs.append(acc)\n",
    "        \n",
    "        # run it on the test data\n",
    "        test_iter = data.batch_iter(test_data, BATCH_SIZE, False)\n",
    "        xent, acc = run_epoch(test_iter, input_pl, target_pl,  # data\n",
    "                              [hs_net_loss, hs_net_acc],  # things to report back on\n",
    "                              tf.no_op(),  # no training\n",
    "                              sess)  # session to run it all\n",
    "        print('\\r~~~Test: xent {}, accuracy {}'.format(xent, acc))\n",
    "        hs_test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10ff74978>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTBj5fyqbKDHxmlqDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ff74a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(train_accs))\n",
    "plt.plot(x, hs_train_accs, ls='dashed', label='train')\n",
    "plt.plot(x, hs_test_accs, label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for extreme laziness, copy and paste to test with the proper binarization and fake gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarised: 1120910 parameters\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# can't do this one on GPU because I haven't done a GPU implementation of the binary connect\n",
    "# it seems like it shouldn't be hard, but in practive it's turned out to be a big pain\n",
    "# so we'll just pin everything to the cpu\n",
    "\n",
    "with tf.device('/cpu:0')\n",
    "    input_pl = tf.placeholder(tf.float32, shape=[BATCH_SIZE, 784], name='inputs')\n",
    "    target_pl = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name='targets')\n",
    "\n",
    "    with tf.variable_scope('bc_net') as scope:\n",
    "        # default nonlin is hard_sigmoid\n",
    "        bc_net_out = get_full_net(input_pl, 10, DEPTH, WIDTH, RANKS, \n",
    "                                  nonlin=bc_01, scope=scope)\n",
    "        # should have the same number of params\n",
    "        print('binarised: {} parameters'.format(count_params(scope.name)))\n",
    "        bc_net_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            bc_net_out, target_pl)\n",
    "        bc_net_loss = tf.reduce_mean(bc_net_loss)\n",
    "        bc_net_acc = tf.contrib.metrics.accuracy(tf.cast(tf.argmax(bc_net_out, 1), tf.int32),\n",
    "                                                 target_pl)\n",
    "        bc_net_acc = tf.reduce_mean(bc_net_acc)\n",
    "        opt = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "        bc_net_train = opt.minimize(\n",
    "            bc_net_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised\n",
      "Epoch 0:\n",
      "~~Train: xent 0.7666185758138696, accuracy 0.8326499963365496\n",
      "~~~Test: xent 0.7457047777343542, accuracy 0.8300999969244003\n",
      "Epoch 1:\n",
      "~~Train: xent 0.3657683405280113, accuracy 0.8956833295027415\n",
      "~~~Test: xent 0.7640804758761078, accuracy 0.8279999986290931\n",
      "Epoch 2:\n",
      "~~Train: xent 0.2884339405417753, accuracy 0.9189333302776018\n",
      "~~~Test: xent 0.7450263647432439, accuracy 0.8394999948143959\n",
      "Epoch 3:\n",
      "~~Train: xent 0.2627443440632972, accuracy 0.9249499964217345\n",
      "~~~Test: xent 0.3873288920894265, accuracy 0.9060999965667724\n",
      "Epoch 4:\n",
      "~~Train: xent 0.23509742546166915, accuracy 0.931349997719129\n",
      "~~~Test: xent 0.7299170335382223, accuracy 0.8179999980330467\n",
      "Epoch 5:\n",
      "~~Train: xent 0.19192580066194448, accuracy 0.9437999982138475\n",
      "~~~Test: xent 0.21266644791903672, accuracy 0.9465999981760979\n",
      "Epoch 6:\n",
      "~~Train: xent 0.18601151112020792, accuracy 0.944916665405035\n",
      "~~~Test: xent 0.27359937732573597, accuracy 0.93259999781847\n",
      "Epoch 7:\n",
      "~~Train: xent 0.1809715113263034, accuracy 0.9463666655123234\n",
      "~~~Test: xent 0.19986148547235644, accuracy 0.9464999985694885\n",
      "Epoch 8:\n",
      "~~Train: xent 0.17236755146974853, accuracy 0.9488166652123133\n",
      "~~~Test: xent 0.27019321385218065, accuracy 0.9270999959111214\n",
      "~~~~~~~~~~~~~~~~~~0.1205,0.9600~~~~~~~~~~~~~~~~~~~"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-1a4015ab1a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                               \u001b[0;34m[\u001b[0m\u001b[0mbc_net_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc_net_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# things to report back on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                               \u001b[0mbc_net_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# thing to do training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                               sess)  # session to run it all\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rEpoch {}:\\n~~Train: xent {}, accuracy {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# record for plotting later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-a224d0cc44eb>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, input_var, target_var, metrics, train_op, session)\u001b[0m\n\u001b[1;32m      6\u001b[0m         feed = {input_var: image_batch,\n\u001b[1;32m      7\u001b[0m                 target_var: label_batch}\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# print some stuff so we know something is happening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now train\n",
    "bc_train_accs, bc_test_accs = [], []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print('initialised')\n",
    "    train_data = data.get_data('train', 60000)\n",
    "    # fix the data from RNN format to feed-forward shapes\n",
    "    # it's a tuple, so redo the sequence\n",
    "    train_data = [train_data[0].reshape((-1, 784)), train_data[1]]\n",
    "    test_data = data.get_data('test', 10000)\n",
    "    test_data = [test_data[0].reshape((-1, 784)), test_data[1]]\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_iter = data.batch_iter(train_data, BATCH_SIZE, False)\n",
    "        xent, acc = run_epoch(train_iter, input_pl, target_pl,  # data\n",
    "                              [bc_net_loss, bc_net_acc],  # things to report back on\n",
    "                              bc_net_train,  # thing to do training\n",
    "                              sess)  # session to run it all\n",
    "        print('\\rEpoch {}:\\n~~Train: xent {}, accuracy {}'.format(epoch, xent, acc))\n",
    "        # record for plotting later\n",
    "        bc_test_accs.append(acc)\n",
    "        \n",
    "        # run it on the test data\n",
    "        test_iter = data.batch_iter(test_data, BATCH_SIZE, False)\n",
    "        xent, acc = run_epoch(test_iter, input_pl, target_pl,  # data\n",
    "                              [bc_net_loss, bc_net_acc],  # things to report back on\n",
    "                              tf.no_op(),  # no training\n",
    "                              sess)  # session to run it all\n",
    "        print('\\r~~~Test: xent {}, accuracy {}'.format(xent, acc))\n",
    "        bc_test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9000cbf2fd8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbc_train_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc_train_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc_test_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pfcmathews/.pyenv/versions/3.5.0/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must have same first dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQpLUZFBIkpr+PzHlOQjhDME4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11763d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(bc_train_accs))\n",
    "plt.plot(x, bc_train_accs, ls='dashed', label='train')\n",
    "plt.plot(x, bc_test_accs, label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Note that a fully connected net, even with just two layers, will probably get 97-98% with\n",
    "this many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
