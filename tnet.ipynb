{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rnndatasets.sequentialmnist as mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see if this isn't a terrible idea, we are going to construct a deep feedforward network where each weight matrix is a slice of a 3-tensor, which is expressed in a decomposed form.\n",
    "\n",
    "That is to say, the middle part, because we are going to use a decomposition which requires them all to be the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensor_layers(input_var, depth, width, rank, nonlinearity=tf.nn.relu):\n",
    "    # first we get the three parameter matrices as per the CP decomp\n",
    "    A = tf.get_variable('A', shape=[width, rank])\n",
    "    B = tf.get_variable('B', shape=[depth, rank])\n",
    "    C = tf.get_variable('C', shape=[width, rank])\n",
    "    # now we loop through and construct the layers\n",
    "    # the biases are not part of the decomp\n",
    "    layer_in = input_var\n",
    "    for layer in range(depth):\n",
    "        with tf.variable_scope('tensor_layer_{}'.format(layer)):\n",
    "            weights = tf.matmul((A * B[layer, :]), C, transpose_b=True)\n",
    "            bias = tf.get_variable('bias_{}'.format(layer), \n",
    "                                   initializer=tf.constant_initializer(0.0),\n",
    "                                   shape=[width])\n",
    "            layer_in = tf.nn.bias_add(tf.matmul(layer_in, weights), bias)\n",
    "            layer_in = nonlinearity(layer_in)\n",
    "    return layer_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def affine(input_var, new_size, name):\n",
    "    with tf.variable_scope(name):\n",
    "        input_size = input_var.get_shape()[1].value\n",
    "        weights = tf.get_variable('weights', shape=[input_size, new_size])\n",
    "        bias = tf.get_variable('bias', shape=[new_size], \n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        return tf.nn.bias_add(tf.matmul(input_var, weights), bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxine(activations):\n",
    "    in_shape = activations.get_shape()[1].value\n",
    "    a = tf.get_variable('a', shape=[in_shape],\n",
    "                        initializer=tf.constant_initializer(1.0))\n",
    "    b = tf.get_variable('b', shape=[in_shape],\n",
    "                        initializer=tf.constant_initializer(1.0))\n",
    "    c = tf.get_variable('c', shape=[in_shape],\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    a_acts = a * (activations - c)\n",
    "    b_acts = b * (activations - c)\n",
    "    \n",
    "    return tf.maximum(a_acts, b_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orthonormal_init():\n",
    "    def _on_init(shape, dtype=tf.float32):\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError('nope')\n",
    "        np.random.seed(1234)\n",
    "        mat = np.random.normal(size=shape)\n",
    "        q, _ = np.linalg.qr(mat, mode='complete')\n",
    "        print(shape, q.shape)\n",
    "        return q  # need to be more carfeul about the shapes here\n",
    "    return _on_init\n",
    "\n",
    "def orthogonal_regulariser(beta=1.0):\n",
    "    def o_r(mat):\n",
    "        if len(mat.get_shape()) != 2:\n",
    "            return None\n",
    "        cov = tf.matmul(mat, mat, transpose_b=True)  # careful which way\n",
    "        eye = tf.constant(np.eye(mat.get_shape()[0].value), dtype=tf.float32)\n",
    "        return tf.reduce_sum(tf.square(cov - eye)) * beta\n",
    "    return o_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, name='inputs', shape=[None, 784])\n",
    "targets = tf.placeholder(tf.int32, name='targets', shape=[None])\n",
    "\n",
    "DEPTH = 10\n",
    "RANK = 50\n",
    "WIDTH = 100\n",
    "\n",
    "with tf.variable_scope('net'):\n",
    "    input_proj = maxine(affine(inputs, WIDTH, 'input_layer'))\n",
    "    \n",
    "    # do the cool guy stuff\n",
    "    with tf.variable_scope('tensor_stuff', regularizer=orthogonal_regulariser(0.01)):\n",
    "        t_out = get_tensor_layers(input_proj, DEPTH, WIDTH, RANK, nonlinearity=maxine)\n",
    "    \n",
    "    # and output layer\n",
    "    net_out = affine(t_out, 10, 'output')\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(net_out, targets)\n",
    "loss = tf.reduce_mean(loss) + tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "accuracy = tf.contrib.metrics.accuracy(\n",
    "    tf.cast(tf.argmax(net_out, 1), tf.int32), targets)\n",
    "\n",
    "opt = tf.train.RMSPropOptimizer(0.001)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, labels = mnist.get_data('train', 60000)\n",
    "test_data, test_labels = mnist.get_data('test', 10000)\n",
    "data = data.reshape((-1, 784))\n",
    "test_data = test_data.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, labels, batch_size):\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    \n",
    "    idcs = np.arange(len(data))\n",
    "    np.random.shuffle(idcs)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_idcs = idcs[i*batch_size:(i+1)*batch_size]\n",
    "        yield data[batch_idcs,...], labels[batch_idcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "valid_accs = []\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_steps = 0\n",
    "    \n",
    "    for dbatch, tbatch in batch_iter(data, labels, BATCH_SIZE):\n",
    "        batch_loss, _ = sess.run([loss, train_op],\n",
    "                                 {inputs: dbatch,\n",
    "                                  targets: tbatch})\n",
    "        epoch_loss += batch_loss\n",
    "        epoch_steps += 1\n",
    "        \n",
    "    valid_acc = 0\n",
    "    valid_steps = 0\n",
    "    for dbatch, tbatch in batch_iter(test_data, test_labels, BATCH_SIZE):\n",
    "        batch_acc = sess.run(accuracy, {inputs: dbatch, targets: tbatch})\n",
    "        valid_acc += batch_acc\n",
    "        valid_steps += 1\n",
    "    \n",
    "    valid_accs.append(valid_acc/valid_steps)\n",
    "        \n",
    "    print('\\r~~({:>3}) train loss: {:.5f}~~'.format(epoch+1, epoch_loss/epoch_steps), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_accs)\n",
    "print(max(valid_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(784 * WIDTH + DEPTH*WIDTH*WIDTH + 10*WIDTH)\n",
    "print(78*WIDTH + 10*WIDTH + 2*RANK*WIDTH + RANK*DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
