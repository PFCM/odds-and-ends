{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import accumulate\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations, with various initialisation\n",
    "\n",
    "Let's have a look at the histogram of activations per layer in a reasonably deep NN, with a few different inits/activation functions.\n",
    "\n",
    "First we have to make some neural nets, so let's get that ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_layer(input_var, size, name):\n",
    "    # for now we won't bother with a bias\n",
    "    input_size = input_var.get_shape()[1].value\n",
    "    weights = tf.get_variable(name+'_W', [input_size, size])\n",
    "    \n",
    "    return tf.matmul(input_var, weights)\n",
    "\n",
    "def deep_net(input_var, layers, size, nonlinearity):\n",
    "    \"\"\"Return all activations for a net with `layers` layers each of width `size`\"\"\"\n",
    "    shape = [input_var] + ([size] * layers)\n",
    "    i = 0\n",
    "    def layer(a, b):\n",
    "        i += 1\n",
    "        return nonlinearity(dense_layer(a, b, 'Layer{}'.format(i)))\n",
    "    return accumulate(shape, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some initialisers. The first is the 'common heuristic' shot down in Glorot,\n",
    "$$\n",
    "    w_{ij} \\sim \\mathcal{U}\\left[\\frac{-1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right]\n",
    "$$\n",
    "With $n$ the number of inputs to the layer (number of rows of the current matrix, the way we are handling things above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaled_uniform_init():\n",
    "    def _init(shape, dtype=tf.float32):\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError('I can only handle matrices.')\n",
    "        scale = 1.0 / np.sqrt(shape[0])\n",
    "        return np.random.uniform(-scale, scale, shape, dtype=np.float32)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the proposed 'normalized initialisation', aka Glorot Initialisation (which has variance $\\frac{1}{n_i + n_j}$\n",
    "where the $n$ are the size of weight matrix). We will do a version drawn from a normal distribution with this variance as well, because why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glorot_uniform_init():\n",
    "    def _gu_init(shape, dtype=tf.float32):\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError('need shape of length 2')\n",
    "        scale = np.sqrt(6) / np.sqrt(shape[0] + shape[1])\n",
    "        return np.random.uniform(-scale, scale, shape, dtype=np.float32)\n",
    "    return _gu_init\n",
    "\n",
    "def glorot_normal_init():\n",
    "    def _gn_init(shape, dtype=tf.float32):\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError('need shape of length 2')\n",
    "        # std dev should be sqrt(variance), variance shuold be 2/(shape[0] + shape[1])\n",
    "        scale = np.sqrt(2) / np.sqrt(shape[0] + shape[1])\n",
    "        return np.random.normal(scale=scale, size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in Saxe's orthogonal init, coming soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make some nets, we will run them on some MNIST digits eventually so let's make them that size.\n",
    "We also want to play with a few activation functions, so we're going to eventually make something of a grid and let loose on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [300, 784])  # we want to plot averages over a few inputs\n",
    "\n",
    "with tf.variable_scope('standard', initializer=scaled_uniform_init()):\n",
    "    standard_outs = deep_net(inputs, 5, 500, tf.nn.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
